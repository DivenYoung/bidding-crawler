"""ä»æµè§ˆå™¨å†…å®¹æŠ“å–æ•°æ®çš„å®ç”¨å·¥å…·

ä½¿ç”¨æ­¥éª¤ï¼š
1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é‡‡æ‹›ç½‘æœç´¢é¡µé¢
2. å¤åˆ¶é¡µé¢å†…å®¹ï¼ˆCtrl+A, Ctrl+Cï¼‰
3. å°†å†…å®¹ç²˜è´´åˆ° browser_content.txt æ–‡ä»¶ä¸­
4. è¿è¡Œæ­¤è„šæœ¬è§£ææ•°æ®

æˆ–è€…ï¼š
ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œå®ƒä¼šè‡ªåŠ¨ä½¿ç”¨æµè§ˆå™¨å·¥å…·æŠ“å–æ•°æ®
"""

import sys
sys.path.insert(0, '/home/ubuntu/bidding-crawler/src')

import yaml
import structlog
from datetime import datetime

from crawler.browser_crawler import BrowserCrawler
from data.storage import JSONStorage
from data.models import BiddingInfo
from data.matcher import KeywordMatcher

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.dev.ConsoleRenderer()
    ]
)

logger = structlog.get_logger()


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open('config.yaml', 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("tool.start", message="ä»æµè§ˆå™¨å†…å®¹æŠ“å–æ•°æ®")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = BrowserCrawler(keywords=config['crawler']['keywords'])
    matcher = KeywordMatcher(config['crawler']['keywords'])
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æµè§ˆå™¨å†…å®¹æ–‡ä»¶
    import os
    content_file = '/home/ubuntu/bidding-crawler/browser_content.txt'
    
    if os.path.exists(content_file):
        logger.info("file.found", path=content_file)
        print(f"\nğŸ“„ å‘ç°æµè§ˆå™¨å†…å®¹æ–‡ä»¶: {content_file}")
        print("æ­£åœ¨è§£æ...")
        
        # ä»æ–‡ä»¶è§£æ
        results = crawler.parse_from_file(
            content_file,
            region_filter=config['crawler']['search']['region']
        )
    else:
        logger.info("file.not_found", message="æœªæ‰¾åˆ°æµè§ˆå™¨å†…å®¹æ–‡ä»¶")
        print(f"\nâŒ æœªæ‰¾åˆ°æ–‡ä»¶: {content_file}")
        print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
        print("1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: https://search.bidcenter.com.cn/search?keywords=å¹¿å‘Š,æ ‡è¯†,ç‰Œ,æ ‡å¿—,å®£ä¼ ,æ ,æ–‡åŒ–")
        print("2. ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ")
        print("3. æŒ‰ Ctrl+A å…¨é€‰é¡µé¢å†…å®¹")
        print("4. æŒ‰ Ctrl+C å¤åˆ¶")
        print(f"5. å°†å†…å®¹ç²˜è´´åˆ°æ–‡ä»¶: {content_file}")
        print("6. å†æ¬¡è¿è¡Œæ­¤è„šæœ¬")
        return
    
    logger.info("parse.results", count=len(results))
    
    if not results:
        logger.warning("no_results", message="æœªè§£æåˆ°æ•°æ®")
        print("\nâš ï¸  æœªè§£æåˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹æ ¼å¼")
        return
    
    # è½¬æ¢ä¸º BiddingInfo å¯¹è±¡å¹¶åŒ¹é…å…³é”®å­—
    bidding_items = []
    for item in results:
        # åŒ¹é…å…³é”®å­—
        text = f"{item['title']}"
        matched_keywords = matcher.match(text)
        item['keywords_matched'] = matched_keywords
        
        # åˆ›å»º BiddingInfo å¯¹è±¡
        bidding_info = BiddingInfo(**item)
        bidding_items.append(bidding_info)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"âœ… æˆåŠŸè§£æ {len(bidding_items)} æ¡é¡¹ç›®")
    print(f"{'='*60}")
    
    # æŒ‰åœ°åŒºç»Ÿè®¡
    region_stats = {}
    for item in bidding_items:
        region = item.province or "æœªçŸ¥"
        region_stats[region] = region_stats.get(region, 0) + 1
    
    print("\nğŸ“Š åœ°åŒºåˆ†å¸ƒï¼š")
    for region, count in sorted(region_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {region}: {count} æ¡")
    
    # æŒ‰ä¿¡æ¯ç±»å‹ç»Ÿè®¡
    type_stats = {}
    for item in bidding_items:
        info_type = item.info_type or "æœªçŸ¥"
        type_stats[info_type] = type_stats.get(info_type, 0) + 1
    
    print("\nğŸ“‹ ä¿¡æ¯ç±»å‹åˆ†å¸ƒï¼š")
    for info_type, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {info_type}: {count} æ¡")
    
    # æ˜¾ç¤ºå‰5æ¡ç»“æœ
    print(f"\nğŸ“ å‰5æ¡é¡¹ç›®é¢„è§ˆï¼š")
    for i, item in enumerate(bidding_items[:5], 1):
        print(f"\n{i}. {item.title}")
        print(f"   ç±»å‹: {item.info_type}")
        print(f"   åœ°åŒº: {item.province} {item.city or ''}")
        print(f"   é¢„ç®—: {item.budget_amount or 'æœªçŸ¥'}")
        print(f"   æ—¥æœŸ: {item.publish_date.strftime('%Y-%m-%d') if item.publish_date else 'æœªçŸ¥'}")
        print(f"   å…³é”®å­—: {', '.join(item.keywords_matched) if item.keywords_matched else 'æ— '}")
    
    # è¯¢é—®æ˜¯å¦ä¿å­˜
    print(f"\n{'='*60}")
    save = input("æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“ï¼Ÿ(y/n): ").strip().lower()
    
    if save == 'y':
        storage = JSONStorage(config['storage']['json_path'])
        
        # æ£€æŸ¥æ˜¯å¦é¦–æ¬¡è¿è¡Œ
        is_first = storage.is_first_run()
        
        if is_first:
            # é¦–æ¬¡è¿è¡Œï¼Œå…¨é‡ä¿å­˜
            metadata = {
                "last_full_crawl": datetime.now().isoformat(),
                "total_count": len(bidding_items),
                "keywords": config['crawler']['keywords'],
                "region": config['crawler']['search']['region']
            }
            storage.save(bidding_items, metadata)
            logger.info("data.saved", mode="full", count=len(bidding_items))
            print(f"\nâœ… å·²ä¿å­˜ {len(bidding_items)} æ¡æ•°æ®ï¼ˆå…¨é‡ï¼‰")
        else:
            # å¢é‡ä¿å­˜
            added_count = storage.append(bidding_items)
            logger.info("data.appended", count=added_count)
            print(f"\nâœ… å·²è¿½åŠ  {added_count} æ¡æ–°æ•°æ®")
        
        print(f"\nğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œ Streamlit æŸ¥çœ‹æ•°æ®:")
        print(f"   streamlit run src/ui/app.py")
    else:
        logger.info("data.not_saved")
        print("\næ•°æ®æœªä¿å­˜")
    
    logger.info("tool.complete")


if __name__ == "__main__":
    main()
