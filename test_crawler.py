"""æµ‹è¯•çœŸå®çˆ¬è™«åŠŸèƒ½"""
import sys
sys.path.insert(0, '/home/ubuntu/bidding-crawler/src')

import yaml
import structlog
from datetime import datetime

from crawler.search_crawler import SearchCrawler, CrawlerConfig
from data.storage import JSONStorage
from data.models import BiddingInfo
from data.matcher import KeywordMatcher

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.dev.ConsoleRenderer()
    ]
)

logger = structlog.get_logger()


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open('config.yaml', 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def main():
    """æµ‹è¯•çˆ¬è™«"""
    logger.info("test.start", message="å¼€å§‹æµ‹è¯•çœŸå®çˆ¬è™«")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler_config = CrawlerConfig(
        min_delay=config['crawler']['anti_crawl']['min_delay'],
        max_delay=config['crawler']['anti_crawl']['max_delay'],
        max_retries=config['crawler']['anti_crawl']['max_retries']
    )
    
    crawler = SearchCrawler(
        keywords=config['crawler']['keywords'],
        config=crawler_config
    )
    
    # åˆå§‹åŒ–å…³é”®å­—åŒ¹é…å™¨
    matcher = KeywordMatcher(config['crawler']['keywords'])
    
    logger.info("crawler.initialized", keywords=config['crawler']['keywords'])
    
    # æ‰§è¡Œæœç´¢
    logger.info("crawler.searching", message="æ­£åœ¨æŠ“å–é‡‡æ‹›ç½‘æ•°æ®...")
    results = crawler.search_full(
        time_range="è¿‘ä¸‰æœˆ",
        region=config['crawler']['search']['region'],
        info_types=config['crawler']['search']['info_types']
    )
    
    logger.info("crawler.results", count=len(results))
    
    if not results:
        logger.warning("no_results", message="æœªæŠ“å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é¡µé¢ç»“æ„")
        return
    
    # è½¬æ¢ä¸º BiddingInfo å¯¹è±¡å¹¶åŒ¹é…å…³é”®å­—
    bidding_items = []
    for item in results:
        # åŒ¹é…å…³é”®å­—
        text = f"{item['title']}"
        matched_keywords = matcher.match(text)
        item['keywords_matched'] = matched_keywords
        
        # åˆ›å»º BiddingInfo å¯¹è±¡
        bidding_info = BiddingInfo(**item)
        bidding_items.append(bidding_info)
    
    # æ˜¾ç¤ºå‰3æ¡ç»“æœ
    logger.info("sample_results", message=f"æ˜¾ç¤ºå‰3æ¡ç»“æœ:")
    for i, item in enumerate(bidding_items[:3], 1):
        print(f"\n{'='*60}")
        print(f"é¡¹ç›® {i}:")
        print(f"  ID: {item.id}")
        print(f"  æ ‡é¢˜: {item.title}")
        print(f"  ç±»å‹: {item.info_type}")
        print(f"  å‘å¸ƒæ—¥æœŸ: {item.publish_date.strftime('%Y-%m-%d')}")
        print(f"  åœ°åŒº: {item.province} {item.city or ''} {item.district or ''}")
        print(f"  åŒ¹é…å…³é”®å­—: {', '.join(item.keywords_matched)}")
        print(f"  æ¥æº: {item.source_url}")
    
    # è¯¢é—®æ˜¯å¦ä¿å­˜
    print(f"\n{'='*60}")
    print(f"å…±æŠ“å–åˆ° {len(bidding_items)} æ¡æ•°æ®")
    save = input("\næ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“ï¼Ÿ(y/n): ").strip().lower()
    
    if save == 'y':
        storage = JSONStorage(config['storage']['json_path'])
        
        # ä¿å­˜æ•°æ®
        metadata = {
            "last_full_crawl": datetime.now().isoformat(),
            "total_count": len(bidding_items),
            "keywords": config['crawler']['keywords'],
            "region": config['crawler']['search']['region']
        }
        
        storage.save(bidding_items, metadata)
        
        logger.info("data.saved", 
                   count=len(bidding_items), 
                   path=config['storage']['json_path'])
        print(f"\nâœ… å·²ä¿å­˜ {len(bidding_items)} æ¡æ•°æ®åˆ° {config['storage']['json_path']}")
        print(f"\nğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œ Streamlit æŸ¥çœ‹æ•°æ®:")
        print(f"   streamlit run src/ui/app.py")
    else:
        logger.info("data.not_saved", message="ç”¨æˆ·é€‰æ‹©ä¸ä¿å­˜")
        print("\næ•°æ®æœªä¿å­˜")
    
    logger.info("test.complete")


if __name__ == "__main__":
    main()
